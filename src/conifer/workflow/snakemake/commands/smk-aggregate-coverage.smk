"""Aggregate coverage workflow

## About

Aggregate individual coverage files, grouped by SAMPLESET, where a
sample set is a collection of samples. Two aggregate values are
calculated for each site:

1. count - count the number of samples that have a MINIMUM coverage
   given by the tag ge{MINIMUM} (ge=greater/equal than)
2. sum - aggregate the total coverage over all samples

The results are used to generate histograms and boxplots of the
coverages distributions for each feature (e.g. gene, exon, UTR, etc.).
The distributions are then used to determine accessible regions of the
genome.

## Results

Results are output in data/aggregate_coverage/SAMPLESET.

## Reports

Reports are output in reports/aggregate_coverage/SAMPLESET.

"""
import re
import sys
from conifer.snakemake.config import SchemaFiles
from conifer.snakemake import inputs
from conifer.snakemake import params
from conifer.snakemake.lmod import get_envmodules
from pathlib import Path
from snakemake.utils import validate
import pandas as pd
import pypandoc


##############################
# Configuration
##############################
configfile: Path("config/config.yaml")


config["__doc__"] = pypandoc.convert_text(__doc__, "rst", format="md")


report: "../report/aggregate_coverage/workflow.rst"


validate(config, schema=SchemaFiles.CONFIGURATION_SCHEMA)

samples = pd.read_table(config["samples"])
validate(samples, schema=SchemaFiles.SAMPLES_SCHEMA)

REFERENCE = Path(config["ref"])

FEATURE = [
    "UTR",
    "exon",
    "five_prime_UTR",
    "gene",
    "intergenic",
    "intron",
    "three_prime_UTR",
    "CDS",
    "genome",
    "pseudogene",
    "TE.gene",
]


wildcard_constraints:
    data="data",
    feature=f"({'|'.join(FEATURE)})",
    interim="data/interim",
    reports="reports",
    sample=f"({'|'.join(samples.SM.tolist())})",


# Define main ALL target
ALL = dict(
    readme="data/aggregate_coverage/README.md",
    config="results/config/coverage.config.yaml",
    aggregate_count=[],
    aggregate_sum=[],
    aggregate_contigs=[],
    aggregate_boxplot=[],
)
for conf in config.get("aggregate_coverage", []):
    ALL["aggregate_count"] += expand(
        "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{feature}.hist.{ext}",
        sampleset=conf["sampleset"],
        mapping_quality=conf["mapping_quality"],
        coverage=conf["minimum_coverage"],
        npartitions=100,
        ext=["png"],
        feature=FEATURE,
    )
    ALL["aggregate_sum"] += expand(
        "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{feature}.hist.{ext}",
        sampleset=conf["sampleset"],
        mapping_quality=conf["mapping_quality"],
        npartitions=100,
        ext=["png", "html"],
        feature=FEATURE,
    )
    ALL["aggregate_contigs"] += expand(
        "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.{ext}",
        sampleset=conf["sampleset"],
        mapping_quality=conf["mapping_quality"],
        coverage=conf["minimum_coverage"],
        npartitions=100,
        ext=["png", "html"],
        window_size=conf["window_size"],
    )
    ALL["aggregate_contigs"] += expand(
        "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.{ext}",
        sampleset=conf["sampleset"],
        mapping_quality=conf["mapping_quality"],
        npartitions=100,
        ext=["png", "html"],
        window_size=conf["window_size"],
    )
    ALL["aggregate_boxplot"] += expand(
        "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{plot_type}.{ext}",
        sampleset=conf["sampleset"],
        mapping_quality=conf["mapping_quality"],
        coverage=conf["minimum_coverage"],
        npartitions=100,
        ext=["png", "html"],
        plot_type=["boxplot", "violin"],
    )
    ALL["sample_sum"] = expand(
        "data/aggregate_coverage/{sample}/MQ10/sum.d4",
        sample=["diploid"],
    )


rule all:
    input:
        **ALL,


##############################
# Atomic rules
##############################
rule readme:
    output:
        md="data/aggregate_coverage/README.md",
    input:
        "src/conifer/workflow/snakemake/commands/smk-aggregate-coverage.smk",
    run:
        with open(output.md, "w") as fh:
            prefix = "" if __doc__.startswith("#") else "# "
            fh.write(prefix + __doc__)
            fh.write(
                (
                    "\n## Command\n\nThis file was generated by running\n\n"
                    f"    snakemake {' '.join(sys.argv[1:])}\n"
                )
            )


rule unzip_bed:
    """Make unzipped bed regions file."""
    output:
        bed="data/resources/regions/{feature}.bed",
    input:
        bed="data/resources/regions/{feature}.bed.gz",
    conda:
        "../envs/bedtools.yaml"
    benchmark:
        "benchmarks/unzip_bed/data/resources/regions/{feature}.bed.benchmark.txt"
    log:
        "logs/unzip_bed/data/resources/regions/{feature}.bed.log",
    threads: 1
    shell:
        """zcat {input.bed} > {output.bed}"""


rule d4utils_count_min_coverage_chunk:
    """Count samples with a minimum coverage for a chunk"""
    output:
        d4=temp(
            "{data}/aggregate_coverage/{sampleset}/MQ{MQ}/chunks/count_ge{coverage}.{partition}-of-{npartitions}.d4"
        ),
    input:
        d4=inputs.d4_count_min_coverage_chunk_input,
        bed="{data}/resources/{npartitions}/pabies-2.0.fa.fai.{partition}-of-{npartitions}.sequential.bed",
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/count_ge{coverage}.{partition}-of-{npartitions}.d4.benchmark.txt"
    log:
        temp(
            "logs/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/count_ge{coverage}.{partition}-of-{npartitions}.d4.log"
        ),
    threads: 1
    shell:
        """conifer-d4utils count --min-coverage {wildcards.coverage} -R {input.bed} {input.d4} {output.d4} > {log} 2>&1"""


rule d4utils_count_min_coverage_aggregate:
    """Aggregate sample count with a given minimum coverage over chunks"""
    output:
        d4="{data}/aggregate_coverage/{sampleset}/MQ{MQ}/count_ge{coverage}.p{npartitions}.d4",
    input:
        d4=inputs.d4_count_min_coverage_aggregate_input,
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/count_ge{coverage}.p{npartitions}.d4.benchmark.txt"
    log:
        "logs/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/count_ge{coverage}.p{npartitions}.d4.log",
    threads: 1
    shell:
        """conifer-d4utils concat {input} {output} > {log} 2>&1"""


rule d4utils_sum_coverage_chunk:
    """Sum coverage for a sample set over a chunk"""
    output:
        d4=temp(
            "{data}/aggregate_coverage/{sampleset}/MQ{MQ}/chunks/sum.{partition}-of-{npartitions}.d4"
        ),
    input:
        d4=inputs.d4_from_sampleset_input,
        bed="{data}/resources/{npartitions}/pabies-2.0.fa.fai.{partition}-of-{npartitions}.sequential.bed",
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/sum.{partition}-of-{npartitions}.d4.benchmark.txt"
    log:
        temp(
            "logs/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/sum.{partition}-of-{npartitions}.d4.log"
        ),
    threads: 1
    shell:
        """conifer-d4utils sum --chunk-size 10000000 -R {input.bed} {input.d4} {output.d4} > {log} 2>&1"""


rule d4utils_sum_coverage_aggregate:
    """Aggregate chunks of sampleset sum coverage"""
    output:
        "{data}/aggregate_coverage/{sampleset}/MQ{MQ}/sum.p{npartitions}.d4",
    input:
        inputs.d4_sum_coverage_aggregate_input,
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/sum.p{npartitions}.d4.benchmark.txt"
    log:
        "logs/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/sum_coverage.p{npartitions}.d4.log",
    threads: 1
    shell:
        """conifer-d4utils concat {input} {output} > {log} 2>&1"""


rule sum_coverage_individual_sample:
    """For individual sample the sum coverage is the same as the sample coverage"""
    output:
        "{data}/aggregate_coverage/{sample}/MQ{MQ}/sum.d4",
    input:
        lambda wildcards: storage("file://" + str(Path(f"{wildcards.data}/mosdepth_coverage/MQ{wildcards.MQ}/{wildcards.sample}.per-base.d4").resolve())),
    conda:
        "../envs/storage.yaml",
    benchmark:
        "benchmarks/sum_coverage_individual_sample/{data}/aggregate_coverage/{sample}/MQ{MQ}/sum.d4.benchmark.txt",
    log:
        "logs/sum_coverage_individual_sample/{data}/aggregate_coverage/{sample}/MQ{MQ}/sum.d4.log",
    threads: 1
    shell:
        """cp -d {input} {output}"""


rule d4_hist:
    """Generate histogram for d4 file.

    NB: it turns out that there is a serious bug in d4tools stat
    whenever discontinuous regions are used. For a given region with
    discontinuties, d4tools will consider the region from the smallest
    to the largest coordinate for all regions, thereby merging them.
    Until a fix is available, we will use the custom script bedhist to
    generate the histogram.

    """
    output:
        "{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist",
    input:
        d4="{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4",
        region="{data}/resources/regions/{feature}.bed",
    conda:
        "../envs/d4tools.yaml"
    params:
        maxbin=params.d4_hist_maxbin,
        region=lambda wildcards, input: ""
        if wildcards.feature == "genome"
        else f"-R {input.region}",
    log:
        "logs/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist.log",
    benchmark:
        "logs/benchmark/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist.benchmark.txt"
    threads: 1
    shell:
        "d4tools view {params.region} {input.d4} | bedhist --max-bins {params.maxbin} -o {output} > {log} 2>&1"


rule conifer_plot_hist:
    """Plot histogram"""
    output:
        report(
            "{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist.{ext}",
            caption="../report/aggregate_coverage/hist-plot.rst",
            category="{sampleset}",
            subcategory="Individual histogram for {feature} (MQ{MQ})",
        ),
    input:
        hist="{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist",
    params:
        num_bins=params.conifer_plot_hist_num_bins,
        backend=lambda wildcards: {"png": "mpl", "html": "plotly"}[wildcards.ext],
        max_bin=lambda wildcards: "--max-bin 0.99"
        if wildcards.prefix.startswith("sum")
        else "",
        title=params.aggregate_coverage_plot_title,
        cutoff=lambda wildcards: re.sub(
            ".p[0-9]+", "", re.sub("count_ge", "", wildcards.prefix)
        )
        if wildcards.prefix.startswith("count")
        else "",
        labels=lambda wildcards: f"{wildcards.feature}",
    conda:
        "../envs/plotting.yaml"
    benchmark:
        "benchmarks/conifer_plot_hist/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist.{ext}.benchmark.txt"
    log:
        "logs/conifer_plot_hist/{data}/aggregate_coverage/{sampleset}/MQ{MQ}/{prefix}.d4.{feature}.hist.{ext}.log",
    priority: 100
    threads: 1
    shell:
        """conifer-plot hist {input.hist} -o {output} --title "{params.title}" {params.max_bin} --num-bins {params.num_bins} --backend {params.backend} --show-feature-size --labels {params.labels} > {log} 2>&1
        """


rule conifer_make_regions_create_db:
    """Create gff database file for conifer"""
    output:
        db="data/resources/{prefix}.gff3.db",
    input:
        gz="data/resources/{prefix}.gff3.gz",
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/conifer_make_regions_create_db/data/resources/{prefix}.gff3.db.benchmark.txt"
    log:
        "logs/conifer_make_regions_create_db/data/resources/{prefix}.gff3.db.log",
    threads: 1
    shell:
        """conifer-gffutils create-db {input.gz} {output.db} > {log} 2>&1"""


rule conifer_make_regions:
    """Make regions from database"""
    output:
        bed="data/resources/{prefix}.{feature}.bed",
    input:
        db="data/resources/{prefix}.gff3.db",
    conda:
        "../envs/conifer.yaml"
    benchmark:
        "benchmarks/conifer_make_regions/data/resources/{prefix}.{feature}.bed.benchmark.txt"
    log:
        "logs/conifer_make_regions/data/resources/{prefix}.{feature}.bed.log",
    threads: 1
    shell:
        """conifer-gffutils make-regions --feature {wildcards.feature} --annotate {input.db} {output.bed} > {log} 2>&1"""


rule coverage_in_windows:
    """Calculate coverage in windows"""
    output:
        bed="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/{prefix}.p{npartitions}.d4.{window_size}.contigs.bed.gz",
        tbi="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/{prefix}.p{npartitions}.d4.{window_size}.contigs.bed.gz.tbi",
    input:
        d4="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/{prefix}.p{npartitions}.d4",
        bed=f"data/resources/windows/{REFERENCE.name}.fai.{{window_size}}.chromosomes.bed",
        names=f"data/resources/windows/{REFERENCE.name}.fai.{{window_size}}.chromosomes.bed.names",
    params:
        window_size=lambda wildcards: wildcards.window_size,
    wildcard_constraints:
        prefix=r"(count_ge\d+|sum)",
    envmodules:
        *config["envmodules"]["d4tools"],
    conda:
        "../envs/d4tools.yaml"
    benchmark:
        "benchmarks/coverage_in_windows/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/{prefix}.p{npartitions}.d4.{window_size}.contigs.bed.gz.benchmark.txt"
    log:
        "logs/coverage_in_windows/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/{prefix}.p{npartitions}.d4.{window_size}.contigs.bed.gz.log",
    threads: 1
    shell:
        """d4tools stat -t {threads} -r {input.bed} {input.d4} | bgzip -c > {output.bed} && tabix -p bed {output.bed} > {log} 2>&1"""


rule report_plot_count_contig_coverage:
    """Plot contig coverage for count statistic in windows"""
    output:
        report(
            "{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.{ext}",
            caption="../report/aggregate_coverage/coverage-contig-plot.rst",
            category="{sampleset}",
            subcategory="Contig coverage, MQ{mapping_quality}",
        ),
    input:
        bed="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.bed.gz",
        tbi="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.bed.gz.tbi",
    params:
        backend=lambda wildcards: {"png": "mpl", "html": "plotly"}[wildcards.ext],
        title=params.aggregate_coverage_plot_title,
    conda:
        "../envs/plotting.yaml"
    wildcard_constraints:
        ext="(png|html)",
    benchmark:
        "benchmarks/report_plot_count_contig_coverage/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.{ext}.benchmark.txt"
    log:
        "logs/report_plot_count_contig_coverage/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{window_size}.contigs.{ext}.log",
    threads: 1
    shell:
        """conifer-plot coverage {input.bed} {output} --title '{params.title}' --width 15 --height 6 --backend {params.backend} > {log} 2>&1"""


rule report_plot_sum_contig_coverage:
    """Plot contig coverage for sum statistic in windows"""
    output:
        report(
            "{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.{ext}",
            caption="../report/aggregate_coverage/coverage-contig-plot.rst",
            category="{sampleset}",
            subcategory="Contig coverage (MQ{mapping_quality})",
        ),
    input:
        bed="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.bed.gz",
        tbi="{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.bed.gz.tbi",
    params:
        backend=lambda wildcards: {"png": "mpl", "html": "plotly"}[wildcards.ext],
        title=params.aggregate_coverage_plot_title,
    conda:
        "../envs/plotting.yaml"
    wildcard_constraints:
        ext="(png|html)",
    benchmark:
        "benchmarks/report_plot_sum_contig_coverage/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.{ext}.benchmark.txt"
    log:
        "logs/report_plot_sum_contig_coverage/{data}/aggregate_coverage/{sampleset}/MQ{mapping_quality}/sum.p{npartitions}.d4.{window_size}.contigs.{ext}.log",
    threads: 1
    shell:
        """conifer-plot coverage {input.bed} {output} --title '{params.title}' --width 15 --height 6 --backend {params.backend} > {log} 2>&1"""


rule report_plot_boxplot_hist:
    """Make boxplot of coverage histograms"""
    output:
        report(
            "data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{plot_type}.{ext}",
            caption="../report/aggregate_coverage/boxplot-hist-plot.rst",
            category="{sampleset}",
            subcategory="Boxplot histogram (MQ{mapping_quality})",
        ),
    input:
        expand(
            "data/aggregate_coverage/{{sampleset}}/MQ{{mapping_quality}}/count_ge{{coverage}}.p{{npartitions}}.d4.{feature}.hist",
            feature=FEATURE,
        ),
    params:
        backend=lambda wildcards: {"png": "seaborn", "html": "plotly"}[wildcards.ext],
        title=params.aggregate_coverage_plot_title,
        labels=",".join(FEATURE),
    conda:
        "../envs/plotting.yaml"
    wildcard_constraints:
        plot_type="(boxplot|violin)",
    benchmark:
        "benchmarks/report_plot_boxplot_hist/data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{plot_type}.{ext}.benchmark.txt"
    log:
        "logs/report_plot_boxplot_hist/data/aggregate_coverage/{sampleset}/MQ{mapping_quality}/count_ge{coverage}.p{npartitions}.d4.{plot_type}.{ext}.log",
    priority: 200
    threads: 1
    shell:
        """conifer-plot boxplot-hist {input} -o {output} --plot-type {wildcards.plot_type} --title '{params.title}' --backend {params.backend} --labels "{params.labels}" --show-feature-size > {log} 2>&1"""


rule conifer_notebook_plot:
    """Jupyter notebook compiling plots"""
    output:
        "reports/aggregate_coverage/{sampleset}/conifer-notebook-plot.html",
    conda:
        "../envs/jupyter.yaml"
    log:
        "logs/conifer-notebook-plot/reports/aggregate_coverage/{sampleset}/conifer-notebook-plot.html.log",
    threads: 1
    notebook:
        str(
            Path(config["workdir"])
            / "notebooks/aggregate_coverage/conifer-notebook-plot.py.ipynb"
        )


include: "../rules/config.smk"
include: "../rules/partition_regions.smk"
include: "../rules/windowing.smk"


localrules:
    readme,
    conifer_plot_hist,
    conifer_notebook_plot,
    report_plot_boxplot_hist,
    report_plot_count_contig_coverage,
    report_plot_sum_contig_coverage,
    unzip_bed,
